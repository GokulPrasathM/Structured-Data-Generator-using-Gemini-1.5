{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 83735,
          "databundleVersionId": 9881586,
          "sourceType": "competition"
        },
        {
          "sourceId": 10068980,
          "sourceType": "datasetVersion",
          "datasetId": 6186244,
          "isSourceIdPinned": true
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Sherlock Holmes X Gemini",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "OiNVh4PwYTer"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "gemini_long_context_path = kagglehub.competition_download('gemini-long-context')\n",
        "beastgokul_aimo_book_path = kagglehub.dataset_download('beastgokul/aimo-book')\n",
        "packagemanager_pm_68852765_at_12_04_2024_04_06_22_path = kagglehub.notebook_output_download('packagemanager/pm-68852765-at-12-04-2024-04-06-22')\n",
        "google_gemini_1_5_flash_api_api_gemini_1_5_flash_1_path = kagglehub.model_download('google/gemini-1.5-flash-api/Api/gemini-1.5-flash/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "rP9cKJg0YTeu"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sherlock Holmes X Gemini: The Case of the Missing Formula**\n",
        "\n",
        "This notebook discusses how **Sherlock Holmes** uses the powerful **Gemini 1.5** to handle large and complex datasets. With the ability to process **2 million tokens**, **Gemini** becomes an invaluable tool for Sherlock in solving challenging problems and organizing vast amounts of unstructured data."
      ],
      "metadata": {
        "id": "YJHPkXoWYTew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "[![Gemini 1.5 Overview](https://img.youtube.com/vi/uGq5UEGfE4o/0.jpg)](https://www.youtube.com/watch?v=uGq5UEGfE4o)\n",
        "\n",
        "🔗 [Click here to watch the video on YouTube](https://www.youtube.com/watch?v=uGq5UE out! 🚀\n"
      ],
      "metadata": {
        "id": "NY0nO3DvYTex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sherlock Holmes sat at his desk, surrounded by a disorganized pile of papers, books, and notes. A new case had arrived—a renowned mathematician had vanished, and with him, a groundbreaking formula. But there was a twist: the only clue left behind was a series of unsolved **Olympiad math problems**.\n",
        "\n",
        "\"*Watson, we need that dataset,*\" Sherlock muttered. \"*And we need it fast!*\"\n",
        "\n",
        "\n",
        "The **Olympiad dataset** was scattered across various sources—**PDFs**, handwritten notes, and even a **YouTube playlist**. Manually sorting through this mess would take *too long*.\n",
        "\n",
        "\"*Time is of the essence,*\" Sherlock said, as he turned to **Gemini 1.5**.\n"
      ],
      "metadata": {
        "id": "R0Xrx6KyYTez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The Problem: Too Much Data, Too Little Time**\n",
        "Sorting through mountains of data can feel impossible. Whether it's:\n",
        "\n",
        "\n",
        "* PDFs full of useful information but hard to navigate\n",
        "* Hours of YouTube videos with no easy way to find key points\n",
        "* Scattered notes that don’t seem to connect.\n",
        "\n",
        "Doing this manually takes forever and leads to frustration. That’s where Gemini 1.5 comes in—a tool designed to quickly organize and analyze large amounts of information. It makes finding what you need faster and easier.\n",
        "\n"
      ],
      "metadata": {
        "id": "DbRGg6o1YTe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The Discovery: Meet Gemini 1.5 🚀**\n",
        "Context has come a \"long\" way with Gemini 1.5 🌌✨.\n",
        "\n",
        "Handling a large amount of data has always been a challenge for AI models. Until recently, most Large Language Models (LLMs) could only work with about 100k tokens at a time. This was fine for smaller tasks, but it quickly became a problem as datasets grew larger and more complex.\n",
        "\n",
        "###  **What Makes Gemini 1.5 Different?**\n",
        "\n",
        "Gemini 1.5 changes the game by increasing its token limit to 2 million—20 times more than older models. This means it can process and \"remember\" much larger datasets without relying on extra tools like:\n",
        "\n",
        "RAG (Retrieval-Augmented Generation), or\n",
        "Vector databases to retrieve information.\n",
        "By keeping everything in memory, Gemini 1.5 simplifies workflows and handles even massive datasets with ease. It’s the perfect solution for anyone managing complex data, like Sherlock!"
      ],
      "metadata": {
        "id": "95MY3hgVYTe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.ibb.co/VM1PDk3/Picsart-24-12-02-15-04-21-486.png\" alt=\"Picsart-24-12-02-15-04-21-486\" border=\"0\">\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Data Types Processed by Gemini**\n",
        "\n",
        "The **Gemini** processes a wide variety of data types, demonstrating its versatility across common file formats used in daily tasks.\n",
        "\n",
        "- **PDF**: Portable Document Format (`application/pdf`), commonly used for reports, articles, and documents.\n",
        "- **JavaScript**: Scripts for web applications (`application/x-javascript`, `text/javascript`).\n",
        "- **Python**: Python code files (`application/x-python`, `text/x-python`), often used for programming and data analysis.\n",
        "- **TXT**: Plain text files (`text/plain`), typically used for storing raw data or notes.\n",
        "- **HTML**: HyperText Markup Language files (`text/html`), used for creating web pages.\n",
        "- **CSS**: Cascading Style Sheets files (`text/css`), used for styling web pages.\n",
        "- **Markdown**: Markup language for creating formatted text (`text/md`), popular for documentation.\n",
        "- **CSV**: Comma-Separated Values files (`text/csv`), widely used for data storage and analysis.\n",
        "- **XML**: Extensible Markup Language files (`text/xml`), used for data storage and transport.\n",
        "- **RTF**: Rich Text Format files (`text/rtf`), commonly used for text documents with basic formatting.\n",
        "\n",
        "This diverse set of formats shows how Gemini is capable of handling both structured and unstructured data, making it an ideal solution for various applications across different industries.\n",
        "\n"
      ],
      "metadata": {
        "id": "I0zxgmMWYTe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Solving the Chaos: How Sherlock Uses Gemini 1.5**\n",
        "With Gemini 1.5’s advanced capabilities, Sherlock was no longer bogged down by the overwhelming data. Instead of relying on inefficient methods, Sherlock could directly upload his raw PDFs and YouTube transcriptions, and within seconds, Gemini 1.5 transformed this chaos into organized, actionable data."
      ],
      "metadata": {
        "id": "7OleE5KwYTe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Here’s how Sherlock used Gemini 1.5 to solve his problem**:\n",
        "---\n",
        "\n",
        "<a href=\"https://ibb.co/4tQJ4Zp\"><img src=\"https://i.ibb.co/jfpH3vD/Screenshot-2024-12-04-083515.png\" alt=\"Screenshot-2024-12-04-083515\" border=\"0\"></a>\n",
        "\n",
        "The flowchart illustrates how raw data from different sources—PDFs, YouTube Playlist, and text files—can be processed efficiently using Gemini 1.5. It emphasizes:\n",
        "\n",
        "\n",
        "\n",
        "1. **Diverse Data Source**s: Demonstrates handling various formats (e.g., PDFs, videos, text files) in one pipeline.\n",
        "2. **Centralized Processing**: Shows how Gemini 1.5 processes the data to streamline the workflow.\n",
        "3. **Efficiency with Caching**: The processed data is stored in a cache for quick access, reducing redundant computations.\n",
        "4. **Structured Output**: The result is a well-organized Dataset, ready for analysis or further use.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "###  **1. Setup: Connecting APIs and Configuring Keys**\n",
        "Before diving into data processing, let's set up the environment with the required libraries and API keys. This section connects to YouTube's API for transcript retrieval and configures Gemini 1.5 using Generative AI's SDK."
      ],
      "metadata": {
        "id": "IIRLqDXZYTe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import re\n",
        "import random\n",
        "import google.generativeai as genai\n",
        "\n",
        "\n",
        "\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "\n",
        "API_KEY = user_secrets.get_secret(\"YOUTUBE_API_KEY\")\n",
        "GEMINI_API_KEY = api_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n",
        "genai.configure(api_key = GEMINI_API_KEY)\n",
        "youtube = build('youtube', 'v3', developerKey=API_KEY)"
      ],
      "metadata": {
        "_uuid": "e08ba309-0bae-4145-bf5c-21caa28827c8",
        "_cell_guid": "f079c381-cd6d-4cec-803d-33a4afaf0513",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-01T21:58:46.795852Z",
          "iopub.execute_input": "2024-12-01T21:58:46.796294Z",
          "iopub.status.idle": "2024-12-01T21:58:48.665256Z",
          "shell.execute_reply.started": "2024-12-01T21:58:46.796255Z",
          "shell.execute_reply": "2024-12-01T21:58:48.664Z"
        },
        "id": "ISoo4IWLYTe2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. PDF Upload and Processing**  \n",
        "Gemini 1.5 simplifies the process of handling PDFs. Instead of manually reading and organizing the data, Sherlock can directly upload PDFs, and Gemini 1.5 processes them automatically, extracting the relevant information with ease.\n",
        "\n",
        "#### **Steps for PDF Upload and Processing:**\n",
        "\n",
        "1. **Upload PDF File**  \n",
        "   - Use the `upload_pdf_to_gemini()` function to upload a PDF file. This function takes the file path and MIME type (which defaults to `'application/pdf'`). Once uploaded, it returns details about the file, including a URI to access it.\n",
        "\n",
        "2. **Monitor File Processing**  \n",
        "   - The `wait_for_files_active()` function checks the processing status of uploaded files. It ensures that files are fully processed and ready to use. The function waits for each file to move from the **PROCESSING** state to the **ACTIVE** state, periodically checking the filraises an error.\n"
      ],
      "metadata": {
        "id": "OcSv0W--YTe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Function to upload a PDF file to Gemini\n",
        "def upload_pdf_to_gemini(path, mime_type='application/pdf'):\n",
        "    file = genai.upload_file(path, mime_type=mime_type)\n",
        "    print(f\"Uploaded file '{file.display_name}' as: {file.uri}\")\n",
        "    return file\n",
        "\n",
        "# Function to wait for PDFs to be processed and become active\n",
        "def wait_for_files_active(files):\n",
        "    print(\"Waiting for file processing...\")\n",
        "    for file in files:\n",
        "        while file.state.name == \"PROCESSING\":\n",
        "            print(\".\", end=\"\", flush=True)\n",
        "            time.sleep(10)\n",
        "            file = genai.get_file(file.name)\n",
        "        if file.state.name != \"ACTIVE\":\n",
        "            raise Exception(f\"File {file.name} failed to process\")\n",
        "    print(\"...all files ready!\")"
      ],
      "metadata": {
        "_uuid": "099c1e69-1035-4d05-b666-883f061a01a3",
        "_cell_guid": "69958285-1fc7-4a25-b9c4-ad7fd0dc82e8",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-01T21:58:48.667244Z",
          "iopub.execute_input": "2024-12-01T21:58:48.667759Z",
          "iopub.status.idle": "2024-12-01T21:58:48.676286Z",
          "shell.execute_reply.started": "2024-12-01T21:58:48.667597Z",
          "shell.execute_reply": "2024-12-01T21:58:48.674599Z"
        },
        "id": "jnArNl5kYTe3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "pdf_folder = '/kaggle/input/aimo-book/aimo'\n",
        "\n",
        "pdf_files = os.listdir(pdf_folder)\n",
        "\n",
        "pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
        "\n",
        "print(f\"PDFs to be uploaded: {pdf_files}\")"
      ],
      "metadata": {
        "_uuid": "ce605bdd-bb5d-47ed-994c-4ebba8726a0e",
        "_cell_guid": "d3af1ba9-9078-411e-bb54-a60fa8d7e04f",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-01T22:00:22.335615Z",
          "iopub.execute_input": "2024-12-01T22:00:22.336556Z",
          "iopub.status.idle": "2024-12-01T22:00:22.352572Z",
          "shell.execute_reply.started": "2024-12-01T22:00:22.336507Z",
          "shell.execute_reply": "2024-12-01T22:00:22.351491Z"
        },
        "id": "GAL9AKT7YTe3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. YouTube Playlist Integration**  \n",
        "Sherlock used **Gemini 1.5** to automatically fetch and transcribe video content from YouTube playlists, saving time on manual transcription.\n",
        "\n",
        "#### **Steps for Integration:**\n",
        "\n",
        "1. **Fetch Video IDs and Titles:**  \n",
        "   Use the `get_videos_from_playlist()` function to retrieve video IDs and titles from a playlist.\n",
        "\n",
        "2. **Fetch Video Transcripts:**  \n",
        "   The `get_video_transcript()` function retrieves and cleans the transcript for each video.\n",
        "\n",
        "3. **Save Transcripts to File:**  \n",
        "   The code saves each video’s URL and its transcript to a file named `transcripts.txt`."
      ],
      "metadata": {
        "id": "ZlLkBjiKYTe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get video IDs and Titles from the AIMO Playlist\n",
        "def get_videos_from_playlist(playlist_id):\n",
        "    videos = []\n",
        "    next_page_token = None\n",
        "\n",
        "    while True:\n",
        "        request = youtube.playlistItems().list(\n",
        "            part='contentDetails,snippet',\n",
        "            playlistId=playlist_id,\n",
        "            maxResults=50,\n",
        "            pageToken=next_page_token\n",
        "        )\n",
        "        response = request.execute()\n",
        "\n",
        "        for item in response['items']:\n",
        "            video_id = item['contentDetails']['videoId']\n",
        "            title = item['snippet']['title']\n",
        "            videos.append({'video_id': video_id, 'title': title})\n",
        "\n",
        "        next_page_token = response.get('nextPageToken')\n",
        "\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return videos\n",
        "\n",
        "# Let's get all the videos from the Playlist\n",
        "videos = get_videos_from_playlist(\"PLWg28JU8y6C9MG7A_GrWcBg8dH2fKOEDB\")\n",
        "print(f\"Total videos found: {len(videos)}\")"
      ],
      "metadata": {
        "_uuid": "ccf7e818-8654-473d-8072-26bbbb2cf59b",
        "_cell_guid": "974f39d0-1a51-4232-9ae2-ce60b541d3b0",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-01T21:58:48.677611Z",
          "iopub.execute_input": "2024-12-01T21:58:48.678084Z",
          "iopub.status.idle": "2024-12-01T21:58:49.124475Z",
          "shell.execute_reply.started": "2024-12-01T21:58:48.678025Z",
          "shell.execute_reply": "2024-12-01T21:58:49.123037Z"
        },
        "id": "aAeS-xWAYTe3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fetch transcript for a single video\n",
        "def get_video_transcript(video_id):\n",
        "    try:\n",
        "        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
        "        transcript = transcript_list.find_transcript(['en'])\n",
        "        transcript_data = transcript.fetch()\n",
        "        text = ' '.join([item['text'] for item in transcript_data])\n",
        "        # Clean the text\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# Function to get the video URL\n",
        "def get_video_url(video_id):\n",
        "    return f\"https://www.youtube.com/watch?v={video_id}\"\n",
        "\n",
        "# Retrieve all transcripts and write to a file\n",
        "output_file = 'transcripts.txt'\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    for idx, video in enumerate(videos):\n",
        "        video_id = video['video_id']\n",
        "        video_title = video['title']\n",
        "        video_url = get_video_url(video_id)\n",
        "\n",
        "        transcript = get_video_transcript(video_id)\n",
        "\n",
        "        if transcript:\n",
        "            f.write(f\"{video_url}\\n\")\n",
        "            f.write(f\"{transcript}\\n\\n\")\n",
        "        else:\n",
        "            f.write(f\"{video_url}\\n\")\n",
        "            f.write(\"Transcript not available.\\n\\n\")"
      ],
      "metadata": {
        "_uuid": "68a3b82a-d47a-414e-a198-683956ca933a",
        "_cell_guid": "a17b3fee-9ae4-45f1-9810-77168c808be9",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-01T21:58:49.126083Z",
          "iopub.execute_input": "2024-12-01T21:58:49.126498Z",
          "iopub.status.idle": "2024-12-01T22:00:22.3314Z",
          "shell.execute_reply.started": "2024-12-01T21:58:49.126458Z",
          "shell.execute_reply": "2024-12-01T22:00:22.330174Z"
        },
        "id": "dPeHmyHHYTe4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A Milestone in AI: Gemini 1.5 vs Conventional LLMs**\n",
        "\n",
        "This milestone is significant because traditional LLMs were limited to **100k tokens** for context, which meant relying on external systems like **vector stores** for large datasets. In contrast, **Gemini 1.5** can handle **2 million tokens**, enabling it to store all relevant context directly in memory—eliminating the need for third-party systems.\n",
        "\n",
        "### **See the comparison below:**\n",
        "\n",
        "| **Feature**         | **Conventional LLMs**    | **Gemini 1.5 Pro**            |\n",
        "|---------------------|--------------------------|------------------------------|\n",
        "| **Token Capacity**   | ~100k Tokens             | **2 Million Tokens**         |\n",
        "| **External Data**    | Requires Vector Store    | **No External Data Needed**  |\n",
        "| **Data Handling**    | Limited Context          | **Memory-Optimized**         |\n",
        "\n",
        "With **Gemini 1.5**, Sherlock could now process entire datasets in-memory, making data retrieval faster and more accurate.\n"
      ],
      "metadata": {
        "id": "OfGMVK6XYTe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://ibb.co/JnPVGcr\"><img src=\"https://i.ibb.co/wy1VPdp/Screenshot-2024-11-30-203818.png\" alt=\"Screenshot-2024-11-30-203818\" border=\"0\"></a>"
      ],
      "metadata": {
        "id": "Uhpzo-g0YTe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('models/gemini-1.5-pro-002')"
      ],
      "metadata": {
        "_uuid": "c56dfa7b-64dc-4c5d-97ff-d375390ea744",
        "_cell_guid": "7a6f1e78-5311-4bfe-82c7-ace0c31d2009",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-01T22:00:22.354102Z",
          "iopub.execute_input": "2024-12-01T22:00:22.354434Z",
          "iopub.status.idle": "2024-12-01T22:00:22.360091Z",
          "shell.execute_reply.started": "2024-12-01T22:00:22.354401Z",
          "shell.execute_reply": "2024-12-01T22:00:22.358891Z"
        },
        "id": "fQeAosJKYTe5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the contents of transcripts.txt\n",
        "with open('transcripts.txt', 'r', encoding='utf-8') as file:\n",
        "    contents = file.read()\n",
        "\n",
        "response = model.count_tokens(contents)\n",
        "print(f\"Token Count of Youtube Playlist Videos: {response.total_tokens}\")"
      ],
      "metadata": {
        "_uuid": "45ad1597-5e13-469c-b440-260c7fa59f9a",
        "_cell_guid": "99878676-358c-41a5-ba9b-c545d7f5c12c",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-01T22:00:22.361592Z",
          "iopub.execute_input": "2024-12-01T22:00:22.362068Z",
          "iopub.status.idle": "2024-12-01T22:00:24.328735Z",
          "shell.execute_reply.started": "2024-12-01T22:00:22.36202Z",
          "shell.execute_reply": "2024-12-01T22:00:24.327334Z"
        },
        "id": "OKGAc5SZYTe5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PDF Text Extraction**  \n",
        "Sherlock used this approach to extract text from multiple PDF files automatically.\n",
        "\n",
        "#### **Steps for PDF Extraction:**\n",
        "\n",
        "1. **Extract Text from Each PDF:**  \n",
        "   The `extract_text_from_pdf()` function opens a PDF file, reads its pages, and extracts text. If text is found, it is added to a string. If an error occurs during reading, an empty string is returned.\n",
        "\n",
        "2. **Process Multiple PDFs:**  \n",
        "   The script scans a directory of PDF files, using `glob` to find all `.pdf` files. For each PDF, it extracts the text and appends it to a combined string (`pdf_text`)."
      ],
      "metadata": {
        "id": "dhgFfYpVYTe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import glob\n",
        "import os\n",
        "from PyPDF2.errors import PdfReadError\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as pdf_file:\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "            extracted_text = \"\"\n",
        "            for page in pdf_reader.pages:\n",
        "                text = page.extract_text()\n",
        "                if text:\n",
        "                    extracted_text += text\n",
        "            return extracted_text\n",
        "    except PdfReadError as e:\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        return \"\"\n",
        "\n",
        "# Directory with all PDF files\n",
        "pdf_directory = '/kaggle/input/aimo-book/aimo/*.pdf'\n",
        "\n",
        "# Initialize a string to hold combined text from all PDFs\n",
        "pdf_text = \"\"\n",
        "\n",
        "# Loop through each PDF file in the directory\n",
        "for pdf_file_path in glob.glob(pdf_directory):\n",
        "    if os.path.isfile(pdf_file_path):\n",
        "        text = extract_text_from_pdf(pdf_file_path)\n",
        "        pdf_text += text\n",
        "\n",
        "print(\"Extraction complete.\")\n",
        "pdf_text += contents"
      ],
      "metadata": {
        "_uuid": "5454ed9a-5a4c-4673-be48-4966a7976e39",
        "_cell_guid": "71a759dc-115d-4b31-b905-657f502ed4f8",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-01T22:00:24.330098Z",
          "iopub.execute_input": "2024-12-01T22:00:24.330461Z",
          "iopub.status.idle": "2024-12-01T22:00:50.362756Z",
          "shell.execute_reply.started": "2024-12-01T22:00:24.330426Z",
          "shell.execute_reply": "2024-12-01T22:00:50.361172Z"
        },
        "id": "YioS5ZWJYTe5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.count_tokens(pdf_text)\n",
        "print(response)"
      ],
      "metadata": {
        "_uuid": "dfa05fb4-8ebf-4c96-ac7b-315a3e9c5fbe",
        "_cell_guid": "4a03801b-a351-487c-9955-c87f659132a1",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-01T22:00:50.364637Z",
          "iopub.execute_input": "2024-12-01T22:00:50.365081Z",
          "iopub.status.idle": "2024-12-01T22:00:54.465282Z",
          "shell.execute_reply.started": "2024-12-01T22:00:50.365041Z",
          "shell.execute_reply": "2024-12-01T22:00:54.463784Z"
        },
        "id": "WWgQhzjeYTe6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "with open('textbook.txt', \"w\", encoding=\"utf-8\") as f:\n",
        "      f.write(pdf_text)"
      ],
      "metadata": {
        "_uuid": "dfc8da93-2233-47a8-9e49-ac9b1afa5a5e",
        "_cell_guid": "f42d8c42-ad5f-40cd-af2b-0838c2898a9b",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-01T22:00:54.46729Z",
          "iopub.execute_input": "2024-12-01T22:00:54.467825Z",
          "iopub.status.idle": "2024-12-01T22:00:54.483048Z",
          "shell.execute_reply.started": "2024-12-01T22:00:54.46777Z",
          "shell.execute_reply": "2024-12-01T22:00:54.481581Z"
        },
        "id": "fID7mKYPYTe6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **File Upload and Processing with Gemini 1.5**\n",
        "\n",
        "Sherlock used **Gemini 1.5** to upload and process text files. Here's how it works:\n",
        "\n",
        "#### **Steps for File Upload and Processing:**\n",
        "\n",
        "1. **Upload Text File:**  \n",
        "   The file is uploaded to Gemini using `genai.upload_file()`. This function takes the file path as input and returns a file object.\n",
        "\n",
        "2. **Wait for File Processing:**  \n",
        "   The script waits for the file to finish processing. It repeatedly checks the file's state using `file.state.name` until it transitions from **PROCESSING** to **ACTIVE**.\n",
        "\n",
        "3. **Notify When Processing is Complete:**  \n",
        "   Once the file is processed, the URI of the processed file is printed."
      ],
      "metadata": {
        "id": "Q4WA_jUSYTe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = genai.upload_file(\"textbook.txt\")\n",
        "\n",
        "while file.state.name == 'PROCESSING':\n",
        "\n",
        "  print('Waiting for text file to be processed.')\n",
        "\n",
        "  time.sleep(2)\n",
        "\n",
        "  file = genai.get_file(text_file.name)\n",
        "\n",
        "\n",
        "\n",
        "print(f'Text processing complete: {file.uri}')"
      ],
      "metadata": {
        "_uuid": "f629ac4a-e180-4ced-8b89-f2a1b2137723",
        "_cell_guid": "f2917dd7-f86d-459f-8c51-90e98a617781",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-01T22:00:54.484524Z",
          "iopub.execute_input": "2024-12-01T22:00:54.484884Z",
          "iopub.status.idle": "2024-12-01T22:00:55.502879Z",
          "shell.execute_reply.started": "2024-12-01T22:00:54.484848Z",
          "shell.execute_reply": "2024-12-01T22:00:55.501585Z"
        },
        "id": "RJoaTuVKYTe6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The Magic of Context Caching ✨**\n",
        "\n",
        "In the fast-paced world of AI, processing the same data repeatedly wastes valuable time and computational resources. Enter **context caching**, a revolutionary feature in **Gemini 1.5** that allows you to **reuse structured content** without recalculating it every time.\n",
        "\n",
        "With **context caching**, Gemini 1.5 transforms into a **super-efficient powerhouse**. By storing data and instructions in reusable caches, you can skip the repetitive steps and focus on what matters: generating intelligent, high-quality results.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zLFMq85AYTe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How It Works: Implementing Context Caching in Gemini 1.5**\n",
        "\n",
        "Implementing context caching allows you to store processed content and reuse it efficiently. Here's how it's done:\n",
        "\n",
        "### **Step 1: Create a Cached Content Object**\n",
        "\n",
        "1. **Model**: Set the model to `\"gemini-1.5-flash-002\"`.\n",
        "2. **Unique Identifier**: Use a `display_name` (e.g., 'pdf_file') for cache identification.\n",
        "3. **System Instructions**: Define the task or behavior of the AI (e.g., generating questions from the text).\n",
        "4. **Content**: Add the data you want to store in the cache.\n",
        "5. **TTL (Time-to-Live)**: Specify how long the cache remains active."
      ],
      "metadata": {
        "id": "Aj3Fo9lbYTe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.generativeai import caching\n",
        "import datetime\n",
        "\n",
        "\n",
        "cache = caching.CachedContent.create(\n",
        "\n",
        "    model=\"gemini-1.5-flash-002\",\n",
        "\n",
        "    display_name='pdf_file', # used to identify the cache\n",
        "\n",
        "    system_instruction=(\n",
        "\n",
        "        \"You are a highly skilled educational content creator specializing in generating challenging \"\n",
        "        \"numerical problems from provided texts. Your task is to analyze the text and generate 10 unique \"\n",
        "        \"questions related to numerical calculations, formulas, or concepts. Each question should be accompanied \"\n",
        "        \"by the correct answer and an explanation of the steps involved in reaching the solution.\"\n",
        "        \"Ensure that each generated problem is clear and concise. Provide explanations for the solutions with a \"\n",
        "        \"focus on logical steps, intermediate calculations, and clear formulas. Each batch of generated data should \"\n",
        "        \"follow this structure for all 10 questions: Question, Answer, Explanation. Avoid unnecessary content and keep the focus on generating high-quality educational material.\"\n",
        "\n",
        "  \"\"\"Topic,Question,Answer,Explanation\n",
        "\"Geometry\",\"What is the area of a rectangle with length 12 cm and width 8 cm?\",\"96 cm²\",\"Area of a rectangle = length × width. 12 × 8 = 96 cm².\"\n",
        "\"Geometry\",\"What is the circumference of a circle with a radius of 7 cm?\",\"44 cm (approx.)\",\"Circumference of a circle = 2 × π × radius. Using π ≈ 3.14, 2 × 3.14 × 7 ≈ 44 cm.\"\n",
        "\"Geometry\",\"What is the volume of a cube with a side length of 5 cm?\",\"125 cm³\",\"Volume of a cube = side³. 5³ = 125 cm³.\"\n",
        "\"Geometry\",\"If a triangle has a base of 10 cm and a height of 6 cm, what is its area?\",\"30 cm²\",\"Area of a triangle = 0.5 × base × height. 0.5 × 10 × 6 = 30 cm².\"\n",
        "\"Geometry\",\"What is the length of the hypotenuse in a right triangle with legs of 6 cm and 8 cm?\",\"10 cm\",\"Using the Pythagorean theorem: hypotenuse² = 6² + 8². √(36 + 64) = 10 cm.\"\n",
        "Topic,Question,Answer,Explanation\n",
        "\"Percentages\",\"What is 25% of 80?\",\"20\",\"25% of 80 is calculated as 0.25 * 80 = 20.\"\n",
        "\"Percentages\",\"If a product costs $120 after a 20% discount, what was its original price?\",\"$150\",\"If $120 is 80% of the original price, then the original price = $120 / 0.8 = $150.\"\n",
        "\"Percentages\",\"How much is 15% of 240?\",\"36\",\"15% of 240 is calculated as 0.15 * 240 = 36.\"\n",
        "\"Percentages\",\"What is the increase from 70 to 98 as a percentage?\",\"40%\",\"Percentage increase = ((98 - 70) / 70) * 100 = 40%.\"\n",
        "\"Percentages\",\"What is 10% of 350 minus 5% of 200?\",\"15\",\"10% of 350 is 35, and 5% of 200 is 10. The difference is 35 - 10 = 15.\"\n",
        "\"\"\"\n",
        "    ),\n",
        "\n",
        "    contents=[file],\n",
        "\n",
        "    ttl=datetime.timedelta(minutes=630),\n",
        "\n",
        ")"
      ],
      "metadata": {
        "_uuid": "db798e30-af1f-4ddb-90fa-c3a17cffeac9",
        "_cell_guid": "c570cbed-a359-4458-9df4-fb0bd54c1af1",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-01T22:00:55.50427Z",
          "iopub.execute_input": "2024-12-01T22:00:55.504757Z",
          "iopub.status.idle": "2024-12-01T22:00:59.43558Z",
          "shell.execute_reply.started": "2024-12-01T22:00:55.504694Z",
          "shell.execute_reply": "2024-12-01T22:00:59.434246Z"
        },
        "id": "h9PCYxPgYTe7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Leverage the Cached Content**\n",
        "\n",
        "Once the cache is created, it can be used to initialize a Generative Model. This model accesses the cached context, allowing for rapid and efficient output generation without reprocessing the content every time.\n",
        "\n",
        "By retrieving the preprocessed content directly from the cache, this method significantly reduces runtime and boosts efficiency."
      ],
      "metadata": {
        "id": "tVT8WBKSYTe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel.from_cached_content(cached_content=cache)"
      ],
      "metadata": {
        "_uuid": "bc04748f-b1bb-40c9-b4d1-7912e86b0312",
        "_cell_guid": "c4ef88fd-7bea-4746-9bb7-c58fedd787e7",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-01T22:00:59.437108Z",
          "iopub.execute_input": "2024-12-01T22:00:59.437605Z",
          "iopub.status.idle": "2024-12-01T22:00:59.443865Z",
          "shell.execute_reply.started": "2024-12-01T22:00:59.437551Z",
          "shell.execute_reply": "2024-12-01T22:00:59.442468Z"
        },
        "id": "DVOWUVLlYTe7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Why Do We Need Batching?**\n",
        "\n",
        "When working with large amounts of text, processing everything at once can overwhelm the AI model due to **token limits**. Batching helps us break down large datasets into manageable chunks, ensuring we can handle huge amounts of data without exceeding these limits. 🚀\n",
        "\n",
        "> By using batching, we can process large texts efficiently and still maintain high performance."
      ],
      "metadata": {
        "id": "7LUNc9uTYTe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def process_chunk(chunk):\n",
        "    \"\"\"\n",
        "    Generate dataset from a single chunk using caching and system instructions.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        prompt = f\"\"\"\n",
        "        Create 10 challenging numerical problems based on the following text:\n",
        "\n",
        "        Text:\n",
        "        {chunk}\n",
        "\n",
        "        Each problem should include:\n",
        "        - Topic\n",
        "        - Question\n",
        "        - Answer\n",
        "        - Explanation\n",
        "\n",
        "        Respond in a structured plain text format:\n",
        "        1. Topic: <topic>\n",
        "           Question: <question>\n",
        "           Answer: <answer>\n",
        "           Explanation: <explanation>\n",
        "\n",
        "        Repeat for all 10 problems.\n",
        "        \"\"\"\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text  # Return plain text response\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing chunk: {e}\")\n",
        "        return \"\"  # Return empty string on error\n",
        "\n",
        "def process_text_in_batches(full_text, chunk_size, time_limit, output_file):\n",
        "    \"\"\"\n",
        "    Process text in parallel batches with a time limit and save responses to a text file.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    chunks = [full_text[i:i+chunk_size] for i in range(0, len(full_text), chunk_size)]\n",
        "\n",
        "    with open(output_file, \"a\") as f:\n",
        "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                elapsed_time = time.time() - start_time\n",
        "                if elapsed_time > time_limit:\n",
        "                    print(f\"Time limit reached. Stopping after {i} chunks.\")\n",
        "                    break\n",
        "\n",
        "                print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
        "                result = executor.submit(process_chunk, chunk)\n",
        "                chunk_output = result.result()  # Response text\n",
        "                if chunk_output.strip():  # Check if the response is not empty\n",
        "                    f.write(f\"Chunk {i+1}:\\n\")\n",
        "                    f.write(chunk_output)\n",
        "                    f.write(\"\\n\\n\")  # Separate chunks with a blank line\n",
        "\n",
        "# Main script\n",
        "chunk_size = 10000  # Adjust chunk size as needed\n",
        "time_limit_seconds = 10 * 3600  # 10 hours in seconds\n",
        "output_file = \"dataset.txt\"\n",
        "\n",
        "start_time = time.time()\n",
        "process_text_in_batches(pdf_text, chunk_size, time_limit_seconds, output_file)\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Script completed in {elapsed_time / 3600:.2f} hours.\")\n",
        "print(f\"Dataset saved to '{output_file}'.\")\n"
      ],
      "metadata": {
        "_uuid": "deec1e45-3e01-4ed1-801e-15e0a9b0b68b",
        "_cell_guid": "b7d1439e-52c5-45ff-bc6f-7d47f52df608",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-01T22:00:59.446974Z",
          "iopub.execute_input": "2024-12-01T22:00:59.447397Z",
          "iopub.status.idle": "2024-12-01T23:41:25.418682Z",
          "shell.execute_reply.started": "2024-12-01T22:00:59.447362Z",
          "shell.execute_reply": "2024-12-01T23:41:25.417338Z"
        },
        "id": "ZSxA4W3TYTe8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **⚙️ Transforming Chaos into Order**\n",
        "\n",
        "Sherlock uploaded the *unstructured data* into **Gemini 1.5**, instructing it to process the **PDFs**, **text files**, and **videos**. In moments, **Gemini 1.5** organized the data into a neat, structured format—**questions**, **answers**, and **explanations** were now clearly categorized.\n",
        "\n",
        "With this newfound order, Sherlock could now focus on solving the case.\n",
        "\n",
        "---\n",
        "\n",
        "### **💡 The Breakthrough**\n",
        "\n",
        "With the **dataset** now organized, Sherlock quickly analyzed the problems. The **missing formula** was hidden within the questions. By cross-referencing the problems and solutions, he discovered a hidden **code** that pointed to the missing mathematician’s location.\n",
        "\n",
        "\"*Watson, we’ve cracked it!*\" Sherlock exclaimed. \"*The formula was the key all along!*\"\n"
      ],
      "metadata": {
        "id": "XliVGmdMYTe8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Why Gemini 1.5 is a Game-Changer?**\n",
        "\n",
        "In today’s fast-paced world, where data is growing exponentially, the need for automated systems that can process and organize large volumes of information efficiently is critical. **Gemini 1.5** revolutionizes the way we interact with data, especially in scenarios involving unstructured and semi-structured data, like PDFs, YouTube playlists, and text files.\n",
        "\n",
        "Before Gemini 1.5, Sherlock Holmes struggled to manage vast amounts of unprocessed data. He had to rely on manual methods and external tools, which were not only time-consuming but also prone to human error. With **Gemini 1.5’s 2 million token capacity**, Sherlock was able to store relevant context in memory, eliminating the need for external databases and drastically improving data retrieval times. This made the entire process of organizing, structuring, and generating insights from raw data faster and more accurate.\n",
        "\n",
        "By integrating **context caching** and leveraging the full potential of **Gemini 1.5**, Sherlock streamlined his entire workflow, achieving a level of efficiency that would have been impossible with traditional methods. The ability to handle millions of tokens of data directly in memory transformed Sherlock’s approach to problem-solving, allowing him to tackle even the most complex datasets with ease.\n",
        "\n",
        "In essence, **Gemini 1.5** not only saved time but also enhanced Sherlock’s capability to extract actionable insights, making it an indispensable tool for any modern data-driven process. With **Gemini 1.5**, we see how the future of AI-powered research and data processing lies in systems that are not just reactive, but proactive, turning vast amounts of raw data into usable, organized knowledge almost instantaneously. 🌟\n",
        "\n"
      ],
      "metadata": {
        "id": "map5R03CYTe9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **🎉 The Resolution**\n",
        "\n",
        "Thanks to **Gemini 1.5**, Sherlock was able to piece together the clues and find the missing mathematician. The **crime** was solved, and the formula was recovered.\n",
        "\n",
        "---\n",
        "\n",
        "# **Conclusion**\n",
        "\n",
        "As the data transformed from a jumbled mess into neat, structured insights, Sherlock couldn’t help but let out a satisfied chuckle. \"**Well, Watson,**\" he said, \"**it seems I’ve solved a case that doesn’t require a magnifying glass. Just a little help from technology.**\"\n",
        "\n",
        "\n",
        "This story demonstrates the **power of Gemini 1.5** in transforming *raw, unstructured data* into **valuable, structured insights**. Whether solving a crime or tackling a complex dataset, **organization** and **structure** are key to success. **Gemini 1.5** is the tool you need to turn **chaos** into **clarity**!\n",
        "\n",
        "\n",
        "<a href=\"https://ibb.co/4gw5WCh\"><img src=\"https://i.ibb.co/Vp0PVbf/Leonardo-Phoenix-Create-an-image-of-cartoon-character-aka-bene-3.jpg\" alt=\"Leonardo-Phoenix-Create-an-image-of-cartoon-character-aka-bene-3\" border=\"0\"></a>"
      ],
      "metadata": {
        "id": "CCSjCJJNYTe9"
      }
    }
  ]
}